
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Hello, my name is Sun Woo, a PhD candidate working in theoretical condensed matter physics, co-supervised by Dr. Curt von Keyserlingk of King’s College London and Prof. Austen Lamacraft of Cambridge University.\nI served Republic of Korea’s national service as a ‘Skilled Industry Personnel’, at a medical startup AIRS Medical, where I developed novel machine-learning algorithms for MRI reconstruction and ultrasound vein detection combining MRI Physics, Compressed Sensing, and recent deep learning techniques. I was discharged on November 2021 and continued to work there until April 2023.\nDuring my service, I also did part-time research at Max Planck Institute of Complex Systems (mostly remotely) with Prof. Markus Heyl and Dr. Giuseppe de Tomasi on developing efficient ways of studying the dynamics of bosonic systems in the Many-Body Localized (MBL) regime.\nI completed the Masters of Advanced Studies in Physics at Cambridge University in 2019, and a Bachelors of Science in Physics with Theoretical Physics at Imperial College London in 2018. Please check my CV and Projects page for more details. My Google Scholar can be found here, and my Twitter here.\nIf you would like to get in touch, please feel free to contact me at swk34 [at] cantab [dot] ac [dot] uk.\n","date":1725667200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1725667200,"objectID":"68b6b3d93c9b3620d7854d6d088cfd84","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hello, my name is Sun Woo, a PhD candidate working in theoretical condensed matter physics, co-supervised by Dr. Curt von Keyserlingk of King’s College London and Prof. Austen Lamacraft of Cambridge University.","tags":null,"title":"Sun Woo P. Kim","type":"authors"},{"authors":["Sun Woo P. Kim"],"categories":null,"content":"","date":1725667200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725667200,"objectID":"146b1f0bbef1abb6ba38a3708560ee34","permalink":"https://sunwoo-kim.github.io/en/projects/plantedpolymer/","publishdate":"2024-09-07T00:00:00Z","relpermalink":"/en/projects/plantedpolymer/","section":"projects","summary":" ","tags":null,"title":"The planted directed polymer; inferring a random walk from noisy images","type":"projects"},{"authors":["Sun Woo P. Kim"],"categories":null,"content":"","date":1715558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715558400,"objectID":"396077879081887d3487fe5177a83e7e","permalink":"https://sunwoo-kim.github.io/en/activities/talks/240513-kias/","publishdate":"2024-05-13T00:00:00Z","relpermalink":"/en/activities/talks/240513-kias/","section":"activities","summary":"Slides for talks at Korea Institute of Advanced Studies and Seoul National University.","tags":["Talk"],"title":"The planted directed polymer; inferring a random walk from noisy images","type":"activities"},{"authors":["Sun Woo P. Kim"],"categories":null,"content":" To cite this page @misc{swpkim2024bayesian, author={P. Kim, Sun Woo}, title={Bayesian inference, statistical physics, and the planted directed polymer problem}, year={2024}, howpublished={\\url{https://sunwoo-kim.github.io/en/activities/talks/240409-imperial/}}, note={Accessed: 2024-10-21} } Below are notes for my board talk that I gave on 2024-04-09 at the regular condensed matter theory meeting at Imperial College London. Table of Contents Introduction The planted random-bond Ising model The planted directed polymer problem Introduction Recently, there has been a lot of activity in casting problems in Bayesian inference problems as disordered stat-mech problems, then using tools of spin glasses to solve them, see Zdeborová 1511.02476. Here I wanted to give a quick introduction to this concept.\nIn Bayesian inference, we try to infer state $x$ given some measurements/data $y$. We assume some prior distribution $p(x)$ and a measurement model/likelihood $p(y \\vert x)$ to infer a posterior distribution using Baye’s rule, $$ p(x \\vert y) = \\frac{p(y \\vert x) p(x)}{p(y)}, $$ where the ’evidence’ $p(y) = \\sum_x p(y \\vert x) p(x)$ acts as the normalisation for the posterior. If the state space is large, then this is in general intractable. This is very similar to the partition function in statistical physics.\nIf $y$ came from the ’true’ state $x^*$, then the natural question to ask is, is inference possible. To quantify this question, for continuous variables, we may look at the mean-squared error, $$ \\mathrm{MSE} = \\mathop{\\mathbb{E}}_{x \\sim p(\\cdot \\vert y)} [(x-x^*)^2]. $$ Is there any way we can prove when inference is possible? And, can we have phase transitions, say, in the mean-squared error as signal-to-noise is varied?\nWe will consider an idealised scenario covered in the above reference, called the teacher-student scenario. The teacher generates some state of a system, $x^*$ from the ’true/teacher’s prior’ $p_\\mathrm{T}(x^*)$. Then, the teacher generates some data/measurements $y$ given the state of the system via the teacher’s measurement model/likelihood, $p_\\mathrm{T}(y \\vert x^*)$. The teacher then hands the student the measurements $y$.\nThe student then assumes some prior distribution $p_\\mathrm{S}(x)$ and measurement model $p_\\mathrm{S}(y\\vert x)$ to infer a posterior distribution $p_\\mathrm{S}(x \\vert y)$. Then we can consider the joint distribution, $$ \\begin{aligned} p(x,y,x^*) \u0026amp; = p_\\mathrm{S}(x \\vert y) p_\\mathrm{T}(y \\vert x^*) p_\\mathrm{T}(x^*) \\\\ \u0026amp; = \\frac{p_\\mathrm{S}(y \\vert x)p_\\mathrm{S}(x) p_\\mathrm{T}(y \\vert x^*) p_\\mathrm{T}(x^*)}{p_\\mathrm{S}(y)}. \\end{aligned} $$ Note that $p(x^*)=p_\\mathrm{T}(x^*)$, since we can integrate over $p_\\mathrm{S}(x\\vert y)$ wrt $x$ then $p_\\mathrm{T}(y \\vert x^*)$ wrt $y$. However, $p(x) \\neq p_\\mathrm{S}(x)$, since we can’t perform the integration over $y$ and $x^*$ first in general.\nHowever, we can already immediately notice one thing: if the student’s model is equal to the teacher’s model, that is $S=T$, then $p(x,y,x^*)$ is symmetric with respect to $x \\leftrightarrow x^*$ and therefore $x$ is distributed identically to $x^*$, and $p(x) = p_\\mathrm{T}(x^*=x)$. The $\\mathrm{S}=\\mathrm{T}$ point is called the Bayes optimal point.\nTo make the connection to stat-mech clearer, let’s write out the student’s posterior in a suggestive way: $$ p_\\mathrm{S}(x \\vert y) = \\frac{e^{-\\beta H (x, y)}}{Z(y)}, $$ where $Z(y) = \\sum_x e^{-\\beta H(x, y)}$ and $$ -\\beta H (x \\vert y) = \\ln q_\\mathrm{S}(y \\vert x) + \\ln q_\\mathrm{S}(x). $$ Here I wrote the unnormalised distributions ex. $q_\\mathrm{S}(y \\vert x) \\propto p_\\mathrm{S}(y \\vert x)$ as normalisation is ensured by the partition function. We see that the posterior looks like the Boltzmann probability of disordered system (ex. spin glass), with the ‘data’ $y$ playing the role of disorder with $p_\\mathrm{T}(y) = \\sum_{x^*} p_\\mathrm{T}(y \\vert x^*) p_\\mathrm{T}(x^*)$.\nIn the limit where the data contains no information about the true/teacher’s state, ex. when the data is ‘infinitely’ noisy, we see that $p_\\mathrm{T}(y \\vert x^*) \\rightarrow p_\\mathrm{T}(y)$ really becomes random disorder - this would be a situation where the student believes that there is some information in the data, but the teacher is actually supplying random noise.\nWhen there is information about the true/teacher’s state in the data, this information is said to be ‘planted’ in the disorder. The distribution of $p_\\mathrm{T}(y) = \\sum_{x^*} p_\\mathrm{T}(y \\vert x^*) p_\\mathrm{T}(x^*)$ is the ‘planted distribution’, and $p(x, y, x^*)$ is the ‘planted ensemble’.\nThis is all well and good, but is it actually useful? Let’s look at some examples, to see how existing disordered stat-mech problems map onto inference problems.\nThe planted random-bond Ising model To make things more concrete, let’s consider a particular case of a model considered in the review. We consider $L^2$ people standing in a room in a square lattice formation. To each person, …","date":1712620800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712620800,"objectID":"5a5b99a395fe2f21ff54087486f34d16","permalink":"https://sunwoo-kim.github.io/en/activities/talks/240409-imperial/","publishdate":"2024-04-09T00:00:00Z","relpermalink":"/en/activities/talks/240409-imperial/","section":"activities","summary":"Notes for board talk at regular CMT meeting, Imperial College London","tags":["Talk"],"title":"Bayesian inference, statistical physics, and the planted directed polymer problem","type":"activities"},{"authors":["Sun Woo P. Kim"],"categories":null,"content":" To cite this page @misc{swpkim2024what\u0026#39;s, author={P. Kim, Sun Woo}, title={What\u0026#39;s the deal with free energy?}, year={2023}, howpublished={\\url{https://sunwoo-kim.github.io/en/posts/free-energy/}}, } The term ‘free energy’ confused me for a long time. One one hand, when we learn thermodynamics, people talk about the Helmholtz free energy, $F=U-TS$. On the other hand, in the literature, people seem to describe the logarithm of a partition function to be the ‘free energy’, too. They must be related, but how?\nIn this post, I’m going to try to make the connection clear as I can, starting from the basic assumptions of statistical mechanics. To do this I will essentially re-derive the canonical ensemble, and point out some things along the way. I will also work in units of energy where $k_B=1$. I assume that you have read some notes on statistical physics, but was left confused as I certainly was.\nConsider a system $S$ that is in contact with with a bath $R$, and that $S$ and $R$ together are isolated. Let’s say we know what the total energy, $E_T$, is. We know that $E_T$ is fixed since we said $S$ and $R$ together are isolated.\nLet’s say that $S$ can have energies ${ E_S}$, and that the number of states (some people call it the statistical weight) that $S$ has with energy $E_S$ is $\\Omega_S(E_S)$. $R$ has energies ${E_R}$, and the number of states that $R$ has with energy $E_R$ is $\\Omega_R(E_R)$.\nWe are going to apply the ‘fundamental postulate of statistical mechanics’ on the combined system $S$ and $R$, which just means that we will assume that any state of the combined system, as long as it has total energy $E_T$, is equally likely.\nThe total number of states for the joint system $S$ and $R$ is then 1 $$ \\Omega(E_{T}) = \\sum_{E_S} \\Omega_R(E_R=E_T - E_S) \\Omega_S(E_S). $$ This is counting all the different ways that the total energy can be shared between $S$ and $R$. Entropy is defined as $S := \\ln \\Omega$. So then we have $$ \\Omega(E_T) = \\sum_{E_S} e^{S_R(E_T - E_S) + S_S(E_S)}. $$ Let us assume that the bath is large, and so its energy is much larger than the system’s $E_S \\ll E_T$. Taylor expanding to first order, we have $$ \\Omega (E_T) \\approx \\sum_{E_S} e^{S_S(E_S)} \\exp \\left(S_R(E_T)- \\frac{\\partial S_R}{\\partial E_R} E_S\\right). $$\nInverse temperature is defined as $T^{-1} = \\partial_{E_R} S_R$. So the total number of states is given by Here, we made the assumption that $\\partial_{E_R} S_R$ is positive. This means that we assume that the entropy of the bath at some energy increases with its energy, or put in another way, that the number of states for the bath increase with energy. This is a sensible assumption; we expect there to be more states at higher energies.\n$$ \\Omega (E_T) \\approx e^{S_R(E_T)} \\sum_{E_S} e^{S_S(E_S)} e^{-E_S /T}. $$ The number of states where $S$ has a *particular* energy $E_S$ is given by $$ \\Omega(E_T = E_R+E_S) = \\Omega_R(E_R=E_T-E_S) \\Omega_S(E_S). $$ Then since all states are equally likely, the probability that the system will have energy $E_S$ is just the ratio of number of states for particular energy and the total number of states, $$ \\begin{aligned} p(E_S) \u0026amp; = \\frac{\\Omega_R(E_R=E_T-E_S) \\Omega_S(E_S)}{\\Omega(E_T)} \\\\\\\\ \u0026amp; = \\frac{e^{S_S(E_S)}e^{-E_S /T}}{\\sum_{E_S\u0026#39;} e^{S_S(E_S\u0026#39;)} e^{-E_S\u0026#39;/T}}. \\end{aligned} $$ Since the bath variables are now gone (except sneakily in the temperature), we will drop the subscript $S$ from here on. We also notice another thing: we can write $p(E)$ in terms of the Helmholtz free energy, $F(E) := E - T S(E)$. So we have $$ p(E) = \\frac{e^{-F(E)/T}}{\\sum_{E\u0026#39;} e^{-F(E\u0026#39;)/T}}. $$ In most cases, $F(E)$ is extensive, meaning that $F(E) = N f(E)$, where the free energy density does not scale with system size $f(E) \\sim \\mathcal{O}(1)$. Let\u0026#39;s also define the partition function $Z = \\sum_E e^{-F(E)/T}$. Now consider $E^*$, which is the energy that maximises the Boltzmann weight $e^{-F(E)/T}$ and therefore minimises $F(E)$ and $f(E)$. We can write the probability as $$ p(E) = \\frac{e^{N(f(E^*)-f(E))/T}}{1 + \\sum_{E’ \\neq E^*} e^{-N(f(E’)-f(E^*))/T}}. $$ In the thermodynamic limit, $N \\rightarrow \\infty$, $p(E)$ sharply peaks around $E^*$, since the numerator is zero unless $E=E^*$, and the denominator converges to $1$, since inside the sum, we always have $f(E’) - f(E^*) \u0026gt; 0$.\nWe can also write the partition function in a similar fashion, $$ Z = e^{-F(E^*)/T} \\left(1 + \\sum_{E’ \\neq E^*} e^{-N(f(E’)-f(E^*))/T} \\right). $$ Let’s define a mystery quantity $\\tilde{f} = -\\frac{T}{N} \\ln Z$. Then we have $$ \\begin{align} \\tilde{f} \u0026amp; = f(E^*) - \\frac{T}{N} \\ln \\left(1 + \\sum_{E’ \\neq E^*} e^{-N(f(E’)-f(E^*))/T} \\right) \\\\ \u0026amp; \\mathop{\\rightarrow}_{N \\rightarrow \\infty} f(E^*). \\end{align} $$ So, in the thermodynamic limit, we ‘select’ energies that minimise the free energy (density) $f(E)$. But we also see that $f(E^*)$ also tends to $\\tilde{f}=-\\frac{T}{N} \\ln Z$ in the thermodynamic limit. This is why physicists refer to both $F(E)$ and $\\tilde{F} := N …","date":1700092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700092800,"objectID":"feb395c80e57870a5f729b65b96e8fb9","permalink":"https://sunwoo-kim.github.io/en/posts/free-energy/","publishdate":"2023-11-16T00:00:00Z","relpermalink":"/en/posts/free-energy/","section":"posts","summary":"The term ‘free energy’ confused me for a long time. One one hand, when we learn thermodynamics, people talk about the Helmholtz free energy, $F = U - TS$, which is somehow 'minimised' at constant temperature. On the other hand, in the literature, people seem to describe the logarithm of a partition function to be the ‘free energy’, too. They must be related, but how?","tags":null,"title":"What's the deal with free energy?","type":"posts"},{"authors":["Sun Woo P. Kim"],"categories":null,"content":" To cite this page @misc{swpkim2024continuous, author={P. Kim, Sun Woo}, title={Continuous and discrete Fourier transforms}, year={2023}, howpublished={\\url{https://sunwoo-kim.github.io/en/posts/fourier-transforms/}}, } The conventions and relationships between discrete Fourier transforms (DFT), Fourier series, and continuous Fourier transforms (FT), are confusing enough that I decided to write a reference on it.\nThese notes closely follow the appendix of Austen Lamacraft’s notes, but with some different choices in notation to explain each step as clearly as I can.\nDiscrete Fourier transform Consider a function $f_j$ defined for discrete inputs $j = {0, \\dots, N-1}$.\nThen we can define the discrete fourier transform (DFT) by\n$$ \\tilde{f}_n = C(N) \\sum_{j=0}^{N-1} f_j e^{-i 2 \\pi n j/N}, $$\nwhere $C(N)$ is a normalisation constant. Since $\\tilde{f}_n$ is periodic in $n$ with period $N$, there are many choices for the domain for $n$. We could choose $n \\in {0, \\dots, N-1}$, or choose the domain to be symmetric(ish),\n\\begin{equation} \\label{symmetric_domain} n \\in \\begin{dcases} \\left \\{ -\\frac{N}{2}, \\dots, \\frac{N}{2}-1 \\right\\} \u0026amp; N \\text{ even} \\\\ \\left \\{ -\\frac{N-1}{2}, \\dots, \\frac{N-1}{2} \\right\\} \u0026amp; N\\text{ odd}. \\end{dcases} \\end{equation}\nWhichever convention we choose, we can use the identity $\\sum_n e^{+i 2 \\pi n j /N} = N\\delta_{j,0 \\text{mod} N}$, to show that\n$$ f_j = \\tilde C(N) \\sum_{n} \\tilde{f}_n e^{+i 2 \\pi n j/N}, $$\nwhere the normalisation constants $C(N)$, $\\tilde C(N)$ can be chosen to be anything, as long as $C(N) \\tilde C(N) = 1/N$. The popular choice is $C, \\tilde C = 1/\\sqrt{N}$.\nLater, we will want to look at the limits of $N \\rightarrow \\infty$. There we will choose the appropriate normalisations $C(N), \\tilde C(N)$ such that the integrals have a well-defined limit.\n(Of course, the sign in the exponential is also a convention in defining the Fourier transforms, but the convention used here is used almost everywhere, so let’s not be worried about that.)\nAdding space We can consider the case where our function is a function on discrete space $x_j = aj$, where $a$ is the lattice spacing. Then the system size is $L=Na$, and $N$ the number of discrete sptial points. Then the Fourier transform becomes\n$$ \\tilde{f}_n = C(N) \\sum_{j=1}^N f(x_j) e^{-i 2 \\pi n x_j/L}. $$\nWe can also define the wavevector $k_n = 2 \\pi n /L$, and write\n$$ \\tilde{f}(k_n) = {C}(N) \\sum_{j=1}^N f(x_j) e^{-i k_n x_j}, $$\nwhilst the inverse transform becomes\n$$ f(x_j) = \\tilde{C}(N) \\sum_{n} \\tilde{f}(k_n) e^{+i k_n x_j}. $$\nNow, let’s make two choices from this point onwards:\nFirst, let’s choose the symmetric domain for $n$. And, let’s expand the domain for $j$ to be $\\mathbb Z$, and say that $f_j$ is periodic in $N$, $f_{j} = f_{j+N}$. This doesn’t do anything, as we can just look at the function in the domain we were interested in the end. But it does mean that we can also choose the symmetric domain Eq. \\eqref{symmetric_domain} for $j$ as well. $N \\rightarrow \\infty$ and $a \\rightarrow 0$ keeping $Na = L$ fixed Here, we are going to continuous real-space, but fix the length of the system. This will result in a countably infinite wavevector space. We can write the discrete Fourier transform as\n$$ \\tilde{f}(k_n) = {C}(N) \\sum_{j} f(x_j) e^{-i k_n x_j}. $$\nThen, the spacing between positions becomes $\\delta x_j = a$. We can choose $C(N) = a$, so we have the limit\n$$ \\tilde{f}(k_n) = \\int_{-L/2}^{L/2} dx f(x) e^{-i k_n x}, $$\nwhich then means that we require $\\tilde C(N) = 1/L$, so we have the inverse transform as\n$$ f(x) = \\frac{1}{L} \\sum_{n \\in \\mathbb{Z}} \\tilde{f}(k_n) e^{+i k_n x}. $$\nwhere now $n \\in \\mathbb{Z}$.\nThis then is just the Fourier series of a periodic function $f(x)$.\n$N \\rightarrow \\infty$ and $L \\rightarrow \\infty$, keeping $a$ fixed Here, we are going to keep the real-space discrete, but just make it infinitely long. This will result in a continuously varying wavevector space. We have\n$$ \\tilde{f}(k) = {C}(N) \\sum_{j \\in \\mathbb{Z}} f(x_j) e^{-i k x_j}, $$\nand\n$$ f(x_j) = \\tilde{C}(N) \\sum_{n} \\tilde{f}(k_n) e^{+i k_n x_j}. $$\nSince $k_n = 2 \\pi n /L = 2\\pi n /Na$, our spacing between wavevectors goes to zero. We have $\\delta k_n = 2 \\pi /N a$. The most popular convention is to choose $\\tilde C (N) = 1/Na$, so that we have\n$$ f(x_j) = \\int_{-\\pi/a}^{\\pi /a} \\frac{dk}{2 \\pi} \\tilde{f}(k) e^{+i k x_j}. $$\nThis means that the forward Fourier transform must be\n$$ \\tilde{f}(k) = a \\sum_{j \\in \\mathbb{Z}} f(x_j) e^{-i k x_j}. $$\nNormally, in this limit, people work with $a=1$ with $x_j =j$.\nSending $N \\rightarrow \\infty$, $L \\rightarrow \\infty$, $a \\rightarrow 0$ From the expressions for the $N \\rightarrow \\infty$, $L \\rightarrow \\infty$, and $a$ fixed, we just send $a \\rightarrow 0$. Since the spacing between spatial points is $\\delta x_j = a$, we can just write down\n$$ \\tilde{f}(k) = \\int_{-\\infty}^{\\infty} dxf(x) e^{-i k x}, $$\nand\n$$ f(x) = \\int_{-\\infty}^{\\infty} \\frac{dk}{2 \\pi} \\tilde{f}(k) …","date":1693440000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693440000,"objectID":"c67c3a3d5a4b806ecee7c0566ec6a8e9","permalink":"https://sunwoo-kim.github.io/en/posts/fourier-transforms/","publishdate":"2023-08-31T00:00:00Z","relpermalink":"/en/posts/fourier-transforms/","section":"posts","summary":"The conventions and relationships between discrete Fourier transforms (DFT), Fourier series, and continuous Fourier transforms (FT), are confusing enough that I decided to write a reference on it.","tags":null,"title":"Continuous and discrete Fourier transforms","type":"posts"},{"authors":["Sun Woo P. Kim"],"categories":null,"content":" To cite this page @misc{swpkim2024transverse, author={P. Kim, Sun Woo}, title={Transverse field Ising model}, year={2023}, howpublished={\\url{https://sunwoo-kim.github.io/en/posts/tfi-model/}}, note={Accessed: 2024-10-21} } In condensed matter physics, there are myriads of phenomenological models out there, with information about them scattered throughout different papers and resources. In the field of machine learning, there are efforts such as the Model Zoo which tries to organise these models in one place.\nIn this new series, I wanted to write down an overview of some of the models I looked at in my studies, with, where possible, the precise definition of what physicists mean when they say things like ’this model has a $U(1)$ symmetry’, or ’the FM case is dual to the AFM case’, etc., which were confusing to me when I was first reading about them. I will also try to support the discussion with some simulations from exact diagonalisation (ED) or Monté Carlo results. Since websites are interactive, I’ll try to nest the finer details in collapsibles. Lastly, this page may be updated periodically with new information.\nTable of Contents Introduction Hamiltonian Symmetries Phases of the ground state Quantum to classical mapping $d=1$ case Mapping to free fermions Wavefunction Entanglement $d=2$ case Introduction The transverse-field Ising (TFI) model is a prototypical model of a quantum magnet, where quantum fluctuations are used to generate paramagnetism instead of temperature. Another common name is ‘quantum Ising model’, or ’transverse Ising model’ (TIM). There is already a pretty good Wikipedia article about this model, but I wanted to just write it down in a more technical way, and fill in some missing gaps. Hamiltonian Broadly, I will use the following conventions for the TFI model,\n\\begin{equation} \\hat{H} = J\\sum_{\\langle i,j \\rangle} \\hat{\\sigma}^z_i \\hat{\\sigma}^z_j - g \\sum_i \\hat{\\sigma}^x_i - b \\sum_i \\hat{\\sigma}^z_i, \\end{equation} where $\\langle i, j\\rangle$ denotes nearest neighbours, and $\\sigma^\\alpha_i$ are Pauli matrices. We consider periodic boundary conditions (PBC). Without a loss of generalisation, we can set $J=\\pm 1$. Then $J=-1$ corresponds to ferrmomagnetic (FM) couplings, $J=1$ to antiferromagnetic couplings (AFM), respectively. $g$ is the transverse field strength, and $b$ the longitudinal field strength, which we will take as $b=0$ unless stated otherwise. Symmetries For $b=0$, the Hamiltonian is $\\mathbb{Z}_2$ symmetric, in the sense that we can globally send $\\hat{\\sigma}^z_i \\rightarrow - \\hat{\\sigma}^z_i$ and keep the Hamiltonian invariant, or concretely, $[\\hat{H}, \\hat{U}] = 0$ for $\\hat{U} = \\vec{\\prod_i} \\hat{\\sigma}^x_i$. Phases of the ground state The TFI model admits 3 phases: the ordered phase $g\u0026lt;g_c$, the critical/gapless phase $g=g_c$, and the disordered phase $g\u0026gt;g_c$. Quantum to classical mapping The TFI model in spatial dimensions $d$ can be explicitly mapped to a $d+1$ classical Ising model, in the sense that the zero-temperature (ground-state) density matrix can be explicitly mapped to the partition function of the classical Ising model. (click to open) It is well known that $d$-dimensional quantum systems map to $(d+1)$-dimensional classical systems. Here, I’ll show an explicit mapping between $d$-dimensional quantum Ising model of length $L_Q$ with a $(d+1)$-dimensional Ising Hamiltonian, which is very similar to1 but is for general $d$ and with longitudinal field $b$.\nLet $\\hat{H}_0$ be the part of $\\hat{H}$ that only contain $\\hat{\\sigma}^z$’s, and $\\hat{H}_1$ be the rest of the Hamiltonian. Then the quantum partition function is given by\n\\begin{align} Z_Q = \\mathrm{tr} [e^{-\\beta_Q \\hat{H}_0 - \\beta_Q \\hat{H}_1}]. \\end{align}\nFrom Trotter’s theorem, for any two Hermitian operators bounded from below, $\\hat{A}$, $\\hat{B}$, we have $e^{\\hat{A} + \\hat{B}}= \\lim_{L \\rightarrow \\infty} \\left(e^{-\\hat{A}/L} e^{-\\hat{B}/L}\\right)^L$, therefore. defining $\\tau = \\beta_Q / L$,\n\\begin{align} Z_Q = \\sum_\\sigma \\langle \\sigma \\rvert \\lim_{L \\rightarrow \\infty} \\left(e^{-\\tau\\hat{H}_0} e^{-\\tau \\hat{H}_1}\\right)^L \\lvert \\sigma \\rangle. \\end{align}\nInserting identities, we have\n\\begin{align} Z_Q \u0026amp; = \\lim_{L \\rightarrow \\infty} \\sum_{\\sigma^1, \\dots, \\sigma^L} \\prod_{l=1}^{L} \\langle \\sigma^{l+1} \\rvert e^{-\\tau \\hat{H}_1} e^{-\\tau \\hat{H}_0} \\lvert \\sigma^l \\rangle \\end{align}\n\\begin{align} Z_Q \u0026amp; = \\lim_{L \\rightarrow \\infty} \\sum_{\\sigma^1, \\dots, \\sigma^L} \\prod_{l=1}^{L} e^{-\\tau H_0(\\sigma^l)} \\langle \\sigma^{l+1} \\rvert e^{\\tau g \\sum_i \\hat{\\sigma}^x_i } \\lvert \\sigma^l \\rangle, \\end{align}\nwhere $H_0(\\sigma^l) = - J \\sum_{\\langle i, j\\rangle} \\sigma^l_i \\sigma^l_j - b \\sum_i \\sigma_i^l$. Now we can use the identity that $\\langle \\sigma^{l+1}_i \\rvert e^{\\tau g \\hat{\\sigma}^x_i} \\lvert \\sigma^l_i \\rangle = \\Lambda e^{\\gamma \\sigma^{l+1}_i \\sigma^{l}_i}$, where $\\Lambda = \\sqrt{\\sinh(\\tau g)\\cosh(\\tau g)}$ and $\\gamma = -\\frac{1}{2} \\ln \\tanh(\\tau g)$. Therefore …","date":1691971200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700092800,"objectID":"815c9e667263186829c8628516d7a35e","permalink":"https://sunwoo-kim.github.io/en/posts/tfi-model/","publishdate":"2023-08-14T00:00:00Z","relpermalink":"/en/posts/tfi-model/","section":"posts","summary":"The transverse-field Ising (TFI) model is a prototypical model of a quantum magnet, where quantum fluctuations are used to generate paramagnetism instead of temperature.","tags":["Model Zoo"],"title":"Transverse field Ising model","type":"posts"},{"authors":["Sun Woo P. Kim"],"categories":null,"content":" To cite this page @misc{swpkim2024hello, author={P. Kim, Sun Woo}, title={Hello World}, year={2021}, howpublished={\\url{https://sunwoo-kim.github.io/en/posts/hello-world/}}, note={Accessed: 2024-10-21} } Welcome to my new website! I hope to post some expository writeups on my research projects here.\nEDIT 2023-08-14\nNow $\\LaTeX$ should also work, complete with equation numbering,\n\\begin{equation} \\label{eq:gaussian-integral} \\int_{-\\infty}^{\\infty} dx \\frac{e^{-x^2/2\\sigma^2}}{\\sqrt{2 \\pi \\sigma^2}} = 1, \\end{equation} where Eq. \\eqref{eq:gaussian-integral} is the Gaussian integral identity.\nI also added a little snippet that automatically generates bibtex for my posts, so people can cite the posts if they want.\n","date":1618704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691971200,"objectID":"6764dc15ea7b8ee22c2266fbedf1629e","permalink":"https://sunwoo-kim.github.io/en/posts/hello-world/","publishdate":"2021-04-18T00:00:00Z","relpermalink":"/en/posts/hello-world/","section":"posts","summary":"Welcome to my new website!","tags":null,"title":"Hello World","type":"posts"},{"authors":["Sun Woo P. Kim"],"categories":null,"content":" For the general public (click to open and close) When we put espresso and milk together, we expect the two types of particles to mix, eventually resulting in a homogeneous mixture that we call latte. This phenomenon is called ‘thermalization’. In usual circumstances, we expect systems to thermalize, where a ‘system’ is simply a set of governing rules and agents. In our case of coffee and milk, the governing rules would be the physics of fluids, and the agents would be the coffee and milk particles. There are also systems that do not thermalize. Non-interacting systems are a common example of non-thermalizing systems, since the individual particles cannot talk to each other and therefore cannot mix. One important non-interacting system is an ‘Anderson Localized’ (AL) one, where non-interacting particles are ‘localized’, i.e. they do not venture far away from their initial positions. In other words, they retain a memory of their initial states. Naively, we expect that interacting systems to thermalize. What is somewhat unexpected, then, is that in certain situations, even interacting particles can stay localized and not thermalize. These are called ‘Many-Body Localized’ (MBL) systems and are a big part of condensed matter physics research today. Theorists usually study such phenomena on a lattice. A lattice is a simplification of space, so that instead of particles being in continuous positions, they occupy discrete ‘sites’, which are states that particles can be in. In physics, there are two types of fundamental particles. The first are ‘fermions’, which are particles where only one particle can occupy a given site, much like the game of musical chairs. The second are ‘bosons’, where any number of particles can occupy a single site at a time. In the musical chairs analogy, it would be as if multiple players could sit on top of a single chair. Experimentalists have recently been studying MBL systems using bosons. However, both theoretically and computationally, bosons are much harder to solve compared to fermions. Why? Well, if you think about the possible states for fermions, there can either be a particle or no particle per site, so there are $2 \\times 2 \\times \\cdots \\times 2 = 2^L$ possible states, where $L$ is the number of sites. However, for bosons, if there are $N$ particles in total, there can be 0 to $N$ particles per site, so there are $(N+1)^L$ possible states. As you have more particles, there are simply more possible states for bosons compared to fermions. In our work, we developed a way to study bosonic MBL systems efficiently. The method is quite abstract, but can be explained with the following analogy. A non-interacting quantum system can be thought of as a collection of pendulums, each located on a site in the lattice and each with their own frequency. Each pendulum only affects their own weights, each oscillating with their own frequency as they swing, without getting in the way of pendulums in other sites. In the roughest approximation, the interactions will only change the frequencies of these pendulums. This approximation is called ‘Poincaré-Lindstedt’ theory. By applying this theory/principle in our analyses of bosonic MBL systems, we found that even the most basic approximation is enough to show some of the hallmarks of MBL systems. One of the hallmarks of MBL systems is the slow spreading of information due to the interactions of the particles. The above cover photo depicts a lattice. On the left is the non-interacting AL system, and on the right is the interacting MBL system. The x-axis is space, and the y-axis is ‘log-time’, where time increases exponentially as you go further up the graph, from 1 to 10 to 100 and so on. The areas where the lattice/graph is lit up indicates that information was transmitted. We can see that in the case of the AL system, the spread of information is stunted, but in the case of the MBL system, because the particles interact, information is transmitted, albeit very slowly, since you need to wait an exponentially longer time for information to spread to the next site. If you’d like to know more about this work, please check out the PDF link, which links to the pdf at Physics Review B.\nAbstract (click to open and close) Recent experiments in quantum simulators have provided evidence for the Many-Body Localized (MBL) phase in 1D and 2D bosonic quantum matter. The theoretical study of such bosonic MBL, however, is a daunting task due to the unbounded nature of its Hilbert space. In this work, we introduce a method to compute the long-time real-time evolution of 1D and 2D bosonic systems in an MBL phase at strong disorder and weak interactions. We focus on local dynamical indicators that are able to distinguish an MBL phase from an Anderson localized phase. In particular, we consider the temporal fluctuations of local observables, the spatiotemporal behavior of two-time correlators and Out-Of-Time-Correlators (OTOCs). We show that these few-body …","date":1618704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618704000,"objectID":"814db4367a531471bdb556b67639d72c","permalink":"https://sunwoo-kim.github.io/en/projects/mblbosons/","publishdate":"2021-04-18T00:00:00Z","relpermalink":"/en/projects/mblbosons/","section":"projects","summary":" ","tags":null,"title":"Super long-time dynamics of many-body localised bosons","type":"projects"},{"authors":["Sun Woo P. Kim"],"categories":null,"content":" For the general public (click to open and close) Even though the ‘metric tensor’ seems like an exotic object that only exist in general relativity, it’s actually just a tool to describe effective distances. For example, it could be used to describe time that sound waves take, like inside of the Earth! This could be anisotropic, meaning that sound could be faster in one direction than the other. The speed of sound waves are determined by a myriad of factors, such as density and composition. The next (practical) question is, how do we determine the composition given that we have measured some sound waves at certain locations? Given a metric tensor, we have the ‘forward’ equation that predicts how sound waves will travel, but now we are describing an ‘inverse’ problem where we want to determine the metric tensor with essentially infinite parameters given some finite measurements. How do we do this? A basic way to do this is to use finite parameterisation and use regularisation to constrain the model. A better way is to work in function space and calculate functional derivatives, then do gradient descent. Practically speaking, we resolve them via Fourier kernels, but this is fine, since after this we can increase the number of Fourier kernels until the solution converges. We want to impose any assumptions/knowledge we have about the solution. For example, point-wise convergence and some levels of differentiability are probably good assumptions. To constrain solutions to have such properties, we can work in specific functions spaces – Sobolev spaces are such spaces.\nAbstract (click to open and close) In underdetermined inverse problems such as ray-tracing tomography, finite-parametrisation of the model and regularisation of the misfit functional are often used to force a unique solution. However, these measures are unsatisfactory because they are arbitrarily chosen. An alternative method is to use gradient-based optimisation to find a local minimum of the misfit, where the first order gradient of the misfit is required for the scheme and the second order can be used to quantify the constraints on the obtained solution or to potentially improve the scheme. The adjoint method is an effective way of calculating these gradients. We show that, in order to obtain a sensible solution through this scheme, we must work in an appropriate Sobolev space, the space of square-integrable functions with additional continuity constraints. We apply these ideas to the metric tomographic problem, the problem of finding an unknown metric on a manifold given a finite number of geodesics, which is an analogous problem to ray-tracing tomography of surface waves. We numerically implement this method for the linearised version of the problem on a 2-dimensional torus. Additionally, we present the first and second adjoints for the general problem, which can be used to calculate the first and second order gradients of the misfit function.\n","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556668800,"objectID":"6185384ca574728435f400c07313fae6","permalink":"https://sunwoo-kim.github.io/en/projects/sobolev/","publishdate":"2019-05-01T00:00:00Z","relpermalink":"/en/projects/sobolev/","section":"projects","summary":" ","tags":null,"title":"MASt Thesis - Metric tomography with Sobolev gradients","type":"projects"},{"authors":["Sun Woo P. Kim"],"categories":null,"content":" For the general public (click to open and close) You are probably well aware of tiles on the sidewalk or wallpapers with patterns that repeat themselves, potentially forever. There is some kind of symmetry there, but how do we describe it? Mathematicians were able to codify these patterns using a field called ‘group theory’. In group theory, we define symmetries as transformations that leave the pattern the same, in other words, invariant. For example, assuming that the pattern goes on forever, we can shift the repeating pattern of wallpapers by certain directions and amount can retrieve the same exact pattern as before. In physics, systems that repeat themselves forever are called crystals. What’s cool about crystals is that the symmetries of the crystals have a direct impact on how waves propagating in the crystals themselves behave, such as if we were to shine light (famously known as a wave) through it, or disturb it (resulting in a sound wave) and so on. Only knowing the symmetries of the crystal, we can predict whether a wave propagating in the crystal will spread out evenly, or propagate in a highly directed fashion, such as in the cover photo. In wallpapers or sidewalks, there are two different kinds of symmetries. The first is symmorphic symmetry, where there exists two kinds of separable symmetries: translational, shifting the entire pattern, and rotational, only rotating the entire pattern around certain points. Nonsymmorphic symmetries are a bit more complicated, in that you cannot separate out the translational and rotational symmetries; there are certain symmetries that require partial translation, followed by a rotation. In this project, we predicted how certain crystals would behave based on their symmetries, and used their properties to design ‘waveguides’, that are able to steer the waves around in particular directions efficiently. If you like to know more about the work, please check out the PDF.\nAbstract (click to open and close) The study of wave propagation through structured periodic media depends critically upon the periodic lattice from which the medium is constructed. That is unsurprising, but perhaps what is slightly more surprising, is that pieces of pure mathematics play a key role - in particular, group and representation theory. Group theory is the natural language that encodes the symmetries of shape and form. Here we use it to consider a class of $2D$ periodic crystals whose lattice is encoded by nonsymmorphic space groups. These are often overlooked due to their relative complexity compared to the symmorphic space groups. We demonstrate that nonsymmorphic groups have possible practical interest in terms of coalescence of dispersion curves, Dirac points and band-sticking, using both theory and simulation. Once we’ve laid out the group theoretical framework in the context of the nonsymmorphic crystals, we use it to illustrate how accidental degeneracies can arise in symmorphic square lattices. We combine this phenomenon with topological valley effects to design highly-efficient topological waveguides and energy-splitters.\n","date":1534636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1534636800,"objectID":"f92f02b2c075a850d5cbd92b58eda4ea","permalink":"https://sunwoo-kim.github.io/en/projects/nonsymmorphic/","publishdate":"2018-08-19T00:00:00Z","relpermalink":"/en/projects/nonsymmorphic/","section":"projects","summary":" ","tags":null,"title":"Nonsymmorphic crystals and topological waveguiding","type":"projects"},{"authors":["Sun Woo P. Kim"],"categories":null,"content":" For the general public (click to open and close) Growing network models are simple models that model social settings. For example, it could model how much research papers are cited - people tend to cite papers that are already cited (preferential attachment). This effect by itself leads to winner-takes-all effects, with winners randomly chosen due to random fluctuations. We study some of the models and come up with more accurate expressions for certain limits.\nAbstract (click to open and close) Three growing network models (GNMs) are studied numerically using Monte Carlo sampling and analytically using mean-field approximation. The first is the Barabási-Albert (BA)/PurePref model, where new nodes are connected preferentially connected to existing nodes with many preexisting links. The second is the PureRand model, where there is no preference. The third is the Mixed model, where node choice probability is chosen between the two models. Apart from rederivation of known results, our original contribution is as follows. We study the node degree evolution directly instead of the steady state distribution, to find an analytic form more accurate with number of edges added per timestep, $m$. We confirm the results with numerics.\n","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"7bfed36caec578e4947cef1f619b0fde","permalink":"https://sunwoo-kim.github.io/en/projects/networks/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/en/projects/networks/","section":"projects","summary":"","tags":null,"title":"Growing network models","type":"projects"},{"authors":["Sun Woo P. Kim"],"categories":null,"content":" For the general public (click to open and close) The famous Higgs mechanism, where a scalar (Higgs) field chooses one of the degenerate minima, gives particles their mass. In so-called supersymmetric gauge field theories, the space of degenerate minima is called the moduli space. Information about the moduli space is encoded in diagrams called ‘quiver diagrams’ and an algebraic series called the Hilbert series. We studied the structure of these moduli spaces for a particular set of these.\nAbstract (click to open and close) Moduli space of a gauge ﬁeld theory is an abstract space of vacuum expectation values of scalar fields. It is of physical signiﬁcance as a point in this space must be chosen before the masses of particles can be determined. For $\\mathcal{N} = 4$, $d = 2 + 1$ supersymmetric gauge fields theories, whose information can be encoded in quiver diagrams, there are two branches in its moduli space: the Higgs branch and the Coulomb branch, where the latter can be calculated using the monopole formula. In this article, the Coulomb branch of $A_n$, $C_n$, $F_4$, $G_2$ Dynkin quivers and their affine counterparts were studied by calculating their Hilbert series using the monopole formula. In addition to conﬁrming previous predictions, new implications on the choice of ungauging location (ﬁxing one of the phases) were found, which we state as the ungauging hypothesis.\n","date":1518998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1518998400,"objectID":"8b724596b587c0f0e68894ea8beca617","permalink":"https://sunwoo-kim.github.io/en/projects/quivers/","publishdate":"2018-02-19T00:00:00Z","relpermalink":"/en/projects/quivers/","section":"projects","summary":" ","tags":null,"title":"BSc Thesis - Coulomb branch of 𝓝 = 4, 𝑑 = 2 + 1 supersymmetric gauge field theories","type":"projects"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"ecaf0acadd75077acbe19596e39f5f60","permalink":"https://sunwoo-kim.github.io/en/activities/kclmanybodycircle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/en/activities/kclmanybodycircle/","section":"activities","summary":"I run a reading group with KCL maths and physics PhD students, on topics regarding theories of many-body systems. Click the heading for details.","tags":null,"title":"KCL Many Body Circle","type":"activities"}]